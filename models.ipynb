{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abtract\n",
    "Многие веб-серверы уязвимы для HTTP-атак, и их исправление не всегда возможно, особенно для защиты от эксплойтов 0-го дня. Приведу парочку моделей как решение, способное распознавать обычные шаблоны в HTTP-запросах и отклонять те запросы, которые не соответствуют этим нормальным патернам HTTP-запросов. Решение в основном ориентировано на устройства Интернета вещей. Эти устройства обычно поддерживают ограниченный диапазон запросов. Производительность и энергопотребление не позволяют использовать решение для внутренней безопасности, а программное обеспечение может быть сложно обновить. Предложенная система смогла защитить тестовые серверы, отразив большинство входящих атак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "HTTP_METHODS = {\n",
    "    \"GET\": \"GET(.|\\n)+?(?=GET|POST|\\Z)\",\n",
    "    \"POST\": \"POST(.|\\n)+?(?=GET|POST|\\Z)\"\n",
    "}\n",
    "\n",
    "URL_REGEX = \"http.+?(?= )\"\n",
    "BODY_REGEX = \"(?<=\\n\\n).+(?=\\n\\n)\"\n",
    "\n",
    "\n",
    "\n",
    "def parse(path, request_reg: string):\n",
    "    with open(path) as file:\n",
    "        data = file.read()\n",
    "\n",
    "        requests = []\n",
    "        [requests.append(request.group(0)) for request in re.finditer(request_reg, data, re.MULTILINE)]\n",
    "\n",
    "        return requests\n",
    "\n",
    "def parseParamsFromUrl(request):\n",
    "    url = parseUrl(request)\n",
    "    return urlparse.parse_qs(urlparse.urlparse(url).query)\n",
    "\n",
    "def parseUrl(request):\n",
    "    return re.search(URL_REGEX, request).group(0)\n",
    "\n",
    "def parseParamsFromBody(request):\n",
    "    params = {}\n",
    "    body = re.search(BODY_REGEX, request)\n",
    "\n",
    "    if body is not None:\n",
    "        body = body.group(0)\n",
    "\n",
    "        params = urlparse.parse_qs(body)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "class FeatureCalculator(metaclass=ABCMeta):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, s: string, request_type=False):\n",
    "        pass\n",
    "class LengthFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "        return len(s)\n",
    "class LettersFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "        return sum(c.isalpha() for c in s)\n",
    "    \n",
    "class NonAlphaFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "        return sum((not c.isalpha()) for c in s)\n",
    "    \n",
    "class PathLengthFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "\n",
    "        if not request_type:\n",
    "            return 0\n",
    "\n",
    "        return len(parseUrl(s))\n",
    "    \n",
    "class PathNonAlphaFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "\n",
    "        if not request_type:\n",
    "            return 0\n",
    "\n",
    "        return sum((not c.isalpha()) for c in parseUrl(s))\n",
    "\n",
    "\n",
    "class EntropyFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "        return (-1) * sum(\n",
    "            i / len(s) * math.log2(i / len(s))\n",
    "            for i in collections.Counter(s).values())\n",
    "    \n",
    "class DigitsFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "        return sum(c.isdigit() for c in s)\n",
    "class ArgumentsLengthFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "\n",
    "        if not request_type:\n",
    "            return 0\n",
    "\n",
    "        urlParams = parseParamsFromUrl(s)\n",
    "        bodyParams = parseParamsFromBody(s)\n",
    "\n",
    "        params = {**urlParams, **bodyParams}\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        for param in params.items():\n",
    "            length = length + len(param[0])\n",
    "\n",
    "        return length\n",
    "    \n",
    "class ArgumentsNumberFeatureCalculator(FeatureCalculator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def transform(self, s: string, request_type=False):\n",
    "\n",
    "        if not request_type:\n",
    "            return 0\n",
    "\n",
    "        urlParams = parseParamsFromUrl(s)\n",
    "        bodyParams = parseParamsFromBody(s)\n",
    "\n",
    "        params = {**urlParams, **bodyParams}\n",
    "\n",
    "        return len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class AnomalyClassifier():\n",
    "    def __init__(self, training_data_path):\n",
    "        self.training_data_path = training_data_path\n",
    "        self.param_features_allowed = True\n",
    "        self.bodyparams_features_allowed = True\n",
    "\n",
    "        self.feature_calculators = [\n",
    "            LengthFeatureCalculator(),\n",
    "            DigitsFeatureCalculator(),\n",
    "            LettersFeatureCalculator(),\n",
    "            NonAlphaFeatureCalculator(),\n",
    "            EntropyFeatureCalculator(),\n",
    "            ArgumentsLengthFeatureCalculator(),\n",
    "            ArgumentsNumberFeatureCalculator(),\n",
    "            PathLengthFeatureCalculator(),\n",
    "            PathNonAlphaFeatureCalculator()\n",
    "        ]\n",
    "\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def train_all_methods(self, nu=0.1, kernel=\"rbf\", gamma=0.1):\n",
    "        for method, regex in HTTP_METHODS.items():\n",
    "            self.train(method, regex, nu=nu, kernel=kernel, gamma=gamma)\n",
    "\n",
    "    def train_get_method(self):\n",
    "        self.train(HTTP_METHODS[\"GET\"].index(), HTTP_METHODS[\"GET\"])\n",
    "\n",
    "    def train_post_method(self):\n",
    "        self.train(HTTP_METHODS[\"POST\"].index(), HTTP_METHODS[\"POST\"])\n",
    "\n",
    "    def train(self, method: string, regex: string, nu=0.1, kernel=\"rbf\", gamma=0.1):\n",
    "        requests = parse(self.training_data_path, regex)\n",
    "\n",
    "        X = []\n",
    "\n",
    "        print(\"training for \" + method + \" method\")\n",
    "\n",
    "        for request in requests:\n",
    "            X.append(self.calculate_features(request, param_features_allowed=self.param_features_allowed, bodyparam_features_allowed=self.bodyparams_features_allowed))\n",
    "            if len(X) % 5000 == 0:\n",
    "                print(str(len(X)) + \" examples processed\")\n",
    "\n",
    "        X = self.balance_X(X)\n",
    "\n",
    "        classif = OneClassSVM(nu=nu, kernel=kernel, gamma=gamma, cache_size=500)\n",
    "\n",
    "        print(\"fitting the classifier\")\n",
    "        classif.fit(X)\n",
    "        self.classifiers[method] = classif\n",
    "\n",
    "    def classify(self, path):\n",
    "\n",
    "        classification = []\n",
    "\n",
    "        for method, regex in HTTP_METHODS.items():\n",
    "            requests = parse(path, regex)\n",
    "\n",
    "            print(\"test evaluation with \" + method + \" method for \" + str(len(requests)) + \" requests\")\n",
    "\n",
    "            X = []\n",
    "\n",
    "            for request in requests:\n",
    "                X.append(self.calculate_features(request, param_features_allowed=self.param_features_allowed, bodyparam_features_allowed=self.bodyparams_features_allowed))\n",
    "\n",
    "            X = self.balance_X(X)\n",
    "\n",
    "            classification.append(self.classifiers[method].predict(X))\n",
    "\n",
    "        Y = np.concatenate(classification)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def calculate_features(self, request: string, param_features_allowed: bool, bodyparam_features_allowed: bool):\n",
    "        features = []\n",
    "        for calculator in self.feature_calculators:\n",
    "            features.append(calculator.transform(request, True))\n",
    "\n",
    "        if param_features_allowed:\n",
    "            params = parseParamsFromUrl(request)\n",
    "            for param in params.items():\n",
    "                for calculator in self.feature_calculators:\n",
    "                    features.append(calculator.transform(param[0]))\n",
    "\n",
    "        if bodyparam_features_allowed:\n",
    "            params = parseParamsFromBody(request)\n",
    "            for param in params.items():\n",
    "                for calculator in self.feature_calculators:\n",
    "                    features.append(calculator.transform(param[0]))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def balance_X(self, X):\n",
    "        X_processed = np.zeros([len(X), len(max(X, key=lambda x: len(x)))])\n",
    "        for i, j in enumerate(X):\n",
    "            X_processed[i][0:len(j)] = j\n",
    "\n",
    "        return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"../../normalTrafficTraining.txt\"\n",
    "TEST_DATA_NORMAL_PATH = \"../../normalTrafficTest.txt\"\n",
    "TEST_DATA_ANOMALY_PATH = \"../../anomalousTrafficTest.txt\"\n",
    "ANOMALIES_LABEL = -1\n",
    "NORMAL_LABEL = 1\n",
    "\n",
    "class Evaluator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def evaluate_performance(self, nu, kernel, gamma):\n",
    "        print(\"[RUNNING EVALUATION FOR {nu: \" + str(nu) + \", kernel: \" + str(kernel) + \", gamma: \" + str(gamma) + \"}]\")\n",
    "        print(\"[Training phase]\")\n",
    "        classifier = AnomalyClassifier(TRAIN_DATA_PATH)\n",
    "        classifier.train_all_methods(nu=nu, kernel=kernel, gamma=gamma)\n",
    "        print(\"- classifier trained\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        print(\"[Testing phase for ANOMALIES dataset]\")\n",
    "        Y = classifier.classify(TEST_DATA_ANOMALY_PATH)\n",
    "        \n",
    "        anomalies = Y[Y == ANOMALIES_LABEL].size\n",
    "        normal = Y[Y == NORMAL_LABEL].size\n",
    "        \n",
    "        print(\"- results: \")\n",
    "        print(\"  - anomalies ratio: \" + str(anomalies) + \" / \" + str(len(Y)))\n",
    "        print(\"  - anomalies percentage cover: \" + str((anomalies / len(Y)) * 100) + \"%\")\n",
    "        print(\"  - error rate: \" + str((normal / len(Y)) * 100) + \"%\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        print(\"[Testing phase for NORMAL dataset]\")\n",
    "        Y = classifier.classify(TEST_DATA_NORMAL_PATH)\n",
    "        \n",
    "        anomalies = Y[Y == ANOMALIES_LABEL].size\n",
    "        normal = Y[Y == NORMAL_LABEL].size\n",
    "        \n",
    "        print(\"- results: \")\n",
    "        print(\"  - normal ratio: \" + str(normal) + \" / \" + str(len(Y)))\n",
    "        print(\"  - normal percentage cover: \" + str((normal / len(Y)) * 100) + \"%\")\n",
    "        print(\"  - error rate: \" + str((anomalies / len(Y)) * 100) + \"%\")\n",
    "        print(\"-----------------\")\n",
    "        print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUNNING EVALUATION FOR {nu: 0.1, kernel: rbf, gamma: 0.1}]\n",
      "[Training phase]\n",
      "training for GET method\n",
      "5000 examples processed\n",
      "10000 examples processed\n",
      "15000 examples processed\n",
      "20000 examples processed\n",
      "25000 examples processed\n",
      "fitting the classifier\n",
      "training for POST method\n",
      "5000 examples processed\n",
      "fitting the classifier\n",
      "- classifier trained\n",
      "-----------------\n",
      "[Testing phase for ANOMALIES dataset]\n",
      "test evaluation with GET method for 15088 requests\n",
      "test evaluation with POST method for 9580 requests\n",
      "- results: \n",
      "  - anomalies ratio: 23302 / 24668\n",
      "  - anomalies percentage cover: 94.46246148856818%\n",
      "  - error rate: 5.537538511431815%\n",
      "-----------------\n",
      "[Testing phase for NORMAL dataset]\n",
      "test evaluation with GET method for 28000 requests\n",
      "test evaluation with POST method for 8000 requests\n",
      "- results: \n",
      "  - normal ratio: 30022 / 36000\n",
      "  - normal percentage cover: 83.39444444444445%\n",
      "  - error rate: 16.605555555555558%\n",
      "-----------------\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# svm values\n",
    "NU = [0.1] # [0.1, 0.01, 0.001, 0.0001]\n",
    "GAMMA = [0.1] # [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "for nu in NU:\n",
    "    for gamma in GAMMA:\n",
    "        evaluator.evaluate_performance(nu, \"rbf\", gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
